{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc84c0b4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Rating\n",
    "Implemented using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec45dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "import re\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1181a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c318352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test and train data\n",
    "train_data, test_data = datasets.load_dataset('imdb', split=['train','test'])\n",
    "\n",
    "#Split test data into train (20k) and validate (5k)\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "train_data, valid_data = random_split(list(train_data),[20000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27bfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Extract emoticons\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    # Eliminate excessive whitespace and convert text to lowercase\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower())\n",
    "    # Append emoticons at the end, removing the \"nose\" for standardization\n",
    "    text = text + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    #Split by white space\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5965c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens 69006\n"
     ]
    }
   ],
   "source": [
    "#How many unique tokens are in the text corpus?\n",
    "token_counts = Counter()\n",
    "for review in train_data:\n",
    "    text = review['text']\n",
    "    tokens = tokenizer(text)\n",
    "    token_counts.update(tokens)\n",
    "print('number of tokens', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a726f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11558, 26, 736]\n",
      "[11558, 26, 736, 2152]\n"
     ]
    }
   ],
   "source": [
    "#Map each token to a unique integer. In reverse frequency order. 0 and 1 placeholders\n",
    "#Sort counter in reverse frequency order\n",
    "sorted_dict = sorted(\n",
    "    token_counts.items(), key=lambda x:x[1], reverse=True\n",
    ")\n",
    "ordered_dict = OrderedDict(sorted_dict)\n",
    "\n",
    "#Word_index contains word:index pairs\n",
    "word_index = {}\n",
    "counter = 2\n",
    "for word, freq in ordered_dict.items():\n",
    "    word_index[word] = counter\n",
    "    counter += 1\n",
    "\n",
    "#0 reserverd for padding. 1 reserved for unknown words\n",
    "word_index['<pad>'] = 0\n",
    "word_index['<unk>'] = 1\n",
    "\n",
    "#Demonstrate encoding scheme works\n",
    "def word_index_conversion(text):\n",
    "    encoding = []\n",
    "    tokens = tokenizer(text)\n",
    "    for token in tokens:\n",
    "        encoding.append(word_index.get(token,1))\n",
    "    return encoding\n",
    "\n",
    "#Testing\n",
    "print(word_index_conversion(\"Roses are red\"))\n",
    "print(word_index_conversion(\"roSes ARE reD :)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57718664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for review in batch:\n",
    "        text = review['text']\n",
    "        label = review['label']\n",
    "        label_list.append(label)\n",
    "        processed_text = torch.tensor(word_index_conversion(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    #Ensure all sequence in minibatch have same length to store efficiently as tensor\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1353bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a small sample with batchsize of 4\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_data,batch_size=4,shuffle=False, collate_fn=build_dataloader)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "#Length of text_batch is maximum in the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946f517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into batches of size 32\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=build_dataloader)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=build_dataloader)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=build_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5502e7",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "One way to encode the index is via one hot encoding. Results in sparse feature vectors \\\n",
    "Suffer from curse of dimensionality \\\n",
    "A better approach is to map each word to a vector of fixed size with real-valued elements \\\n",
    "-> Advantage: Reduction in dimensionality of the feature space \\\n",
    "-> Extraction of salient features since the embedding layer in an NN can be optimized \\\n",
    "Let n be the number of unique words \\\n",
    "Embedding matrix is of size (n+2) x embedding_dim. Reserve 2 spots for \\<unknown\\> and \\<pad\\> \\\n",
    "Given integer index i, simply look up the row at index i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103b836",
   "metadata": {},
   "source": [
    "# RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a833a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "def save_model(model, path='sentiment_rnn.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model, path='sentiment_rnn.pth'):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "250c8183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(69008, 128, padding_idx=0)\n",
      "  (rnn): RNN(128, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/5, Training Loss: 0.6422, Validation Loss: 0.5808\n",
      "Epoch 2/5, Training Loss: 0.5759, Validation Loss: 0.5958\n",
      "Epoch 3/5, Training Loss: 0.5442, Validation Loss: 0.6860\n",
      "Epoch 4/5, Training Loss: 0.5175, Validation Loss: 0.5292\n",
      "Epoch 5/5, Training Loss: 0.5237, Validation Loss: 0.6161\n",
      "Test Accuracy: 69.87%\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)  # Embedding layer\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # Apply embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        hidden = hidden[-1]  # Take the last layer's hidden state\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(word_index)\n",
    "embed_size = 128  # Size of the embedding vectors\n",
    "hidden_size = 128  # Number of hidden units in RNN\n",
    "output_size = 2  # Positive or negative sentiment\n",
    "num_layers = 2  # Number of RNN layers\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentRNN(vocab_size, embed_size, hidden_size, output_size, num_layers)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_dl, valid_dl, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for text, labels, lengths in train_dl:\n",
    "            text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(text, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        valid_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for text, labels, lengths in valid_dl:\n",
    "                text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "                outputs = model(text, lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {valid_loss / len(valid_dl):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dl, valid_dl, epochs=5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_dl):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, labels, lengths in test_dl:\n",
    "            text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "            outputs = model(text, lengths)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d32a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to RNN 2 Layers Embedding 128 Hidden 128 Epochs 5\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"RNN 2 Layers Embedding 128 Hidden 128 Epochs 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45460b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/miniconda3/envs/quant/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(69008, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Epoch 1/5, Training Loss: 0.5929, Validation Loss: 0.5373\n",
      "Epoch 2/5, Training Loss: 0.5187, Validation Loss: 0.4915\n",
      "Epoch 3/5, Training Loss: 0.4405, Validation Loss: 0.5174\n",
      "Epoch 4/5, Training Loss: 0.4164, Validation Loss: 0.4556\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, dropout=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)  # Embedding layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout)  # Regularization\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # Apply embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        packed_output, (hidden, _) = self.lstm(packed)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        hidden = hidden[-1]  # Take the last layer's hidden state\n",
    "        \n",
    "        # Apply dropout and fully connected layer\n",
    "        output = self.fc(self.dropout(hidden))\n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(word_index)\n",
    "embed_size = 100  # Size of the embedding vectors\n",
    "hidden_size = 128  # Number of hidden units in LSTM\n",
    "output_size = 2  # Positive or negative sentiment\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "dropout = 0.5  # Dropout probability\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentRNN(vocab_size, embed_size, hidden_size, output_size, num_layers, dropout)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_dl, valid_dl, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for text, labels, lengths in train_dl:\n",
    "            text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(text, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        valid_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for text, labels, lengths in valid_dl:\n",
    "                text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "                outputs = model(text, lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {valid_loss / len(valid_dl):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dl, valid_dl, epochs=5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_dl):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, labels, lengths in test_dl:\n",
    "            text, labels, lengths = text.to(device), labels.to(device), lengths.to(device)\n",
    "            outputs = model(text, lengths)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac46948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f2435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f649662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0390de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
